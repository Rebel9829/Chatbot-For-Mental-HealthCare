{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d398c307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 16:22:23.409321: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-05 16:22:23.715169: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-05 16:22:23.715189: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-05 16:22:24.525308: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-05 16:22:24.525374: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-05 16:22:24.525382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import string\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense,GlobalMaxPooling1D,Embedding,Flatten\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2897845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import JSON file\n",
    "data = pd.read_json(r'intents.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18289656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract questions, tags, responses in separate lists\n",
    "tags=[]\n",
    "questions=[]\n",
    "responses={}\n",
    "for intent in data['intents']:\n",
    "    responses[intent['tag']]=intent['responses']\n",
    "    for lines in intent['patterns']:\n",
    "        questions.append(lines)\n",
    "        tags.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c25690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of tags and questions\n",
    "data_new=pd.DataFrame({\"tags\":tags,\"questions\":questions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4578709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "data_new = data_new.drop_duplicates()\n",
    "\n",
    "# Remove non-alphabetic characters\n",
    "data_new['questions'] = data_new['questions'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "\n",
    "# Convert all text to lowercase\n",
    "data_new['questions'] = data_new['questions'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenize the text\n",
    "data_new['questions'] = data_new['questions'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Convert the text back to a string\n",
    "data_new['questions'] = data_new['questions'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a220477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for tokenizing and encoding the data\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenizing the data\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(data_new['questions'])\n",
    "train_data=tokenizer.texts_to_sequences(data_new['questions'])\n",
    "\n",
    "# Padding the data to make all arrays of equal size\n",
    "x_train=pad_sequences(train_data)\n",
    "\n",
    "# Encoding the labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "y_train=le.fit_transform(data_new['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f0a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output shape\n",
    "input_shape=x_train.shape[1]\n",
    "vocabulary=len(tokenizer.word_index)\n",
    "output_length=le.classes_.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d12a41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 16:22:38.661739: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-05 16:22:38.661761: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-05 16:22:38.661777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (prathmesh-G3-3500): /proc/driver/nvidia/version does not exist\n",
      "2023-04-05 16:22:38.662158: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 5s 86ms/step - loss: 3.7922 - accuracy: 0.1279\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.4782 - accuracy: 0.1652\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.3800 - accuracy: 0.1642\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.1815 - accuracy: 0.1733\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.0835 - accuracy: 0.1742\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 2.9719 - accuracy: 0.2214\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 2.8500 - accuracy: 0.2514\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.6610 - accuracy: 0.2949\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 2.5168 - accuracy: 0.3249\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 2.3462 - accuracy: 0.3639\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 2.1711 - accuracy: 0.4083\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.0295 - accuracy: 0.4583\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 1.8930 - accuracy: 0.4900\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 1.7652 - accuracy: 0.5272\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 1.6731 - accuracy: 0.5463\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 1.5497 - accuracy: 0.5962\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 1.4521 - accuracy: 0.6216\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 1.3805 - accuracy: 0.6298\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 1.3078 - accuracy: 0.6552\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 1.2250 - accuracy: 0.6842\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 1.1631 - accuracy: 0.7033\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 1.1351 - accuracy: 0.7151\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 1.0775 - accuracy: 0.7414\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 1.0259 - accuracy: 0.7577\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.9869 - accuracy: 0.7523\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 1.0352 - accuracy: 0.7377\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.9857 - accuracy: 0.7586\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.8831 - accuracy: 0.7804\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.8437 - accuracy: 0.8004\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.8147 - accuracy: 0.8058\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.7890 - accuracy: 0.8058\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.7551 - accuracy: 0.8094\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.7318 - accuracy: 0.8140\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.7167 - accuracy: 0.8167\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6895 - accuracy: 0.8267\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.6706 - accuracy: 0.8358\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6586 - accuracy: 0.8258\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.6358 - accuracy: 0.8403\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6188 - accuracy: 0.8457\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6010 - accuracy: 0.8548\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.5846 - accuracy: 0.8584\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.5701 - accuracy: 0.8566\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.5583 - accuracy: 0.8566\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.5477 - accuracy: 0.8648\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.5315 - accuracy: 0.8639\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.5158 - accuracy: 0.8730\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.5064 - accuracy: 0.8775\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.4912 - accuracy: 0.8784\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.4883 - accuracy: 0.8757\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.5669 - accuracy: 0.8566\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.6584 - accuracy: 0.8285\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.5162 - accuracy: 0.8675\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.4655 - accuracy: 0.8902\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.4455 - accuracy: 0.8875\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.4281 - accuracy: 0.9002\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.4168 - accuracy: 0.9038\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.4066 - accuracy: 0.9011\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.3997 - accuracy: 0.9038\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.3904 - accuracy: 0.9083\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.3842 - accuracy: 0.9074\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.3772 - accuracy: 0.9156\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.3677 - accuracy: 0.9065\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.3567 - accuracy: 0.9174\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.3483 - accuracy: 0.9165\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.3413 - accuracy: 0.9192\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.3326 - accuracy: 0.9265\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.3336 - accuracy: 0.9256\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.3250 - accuracy: 0.9192\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.3128 - accuracy: 0.9283\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.3069 - accuracy: 0.9328\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.2987 - accuracy: 0.9328\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2938 - accuracy: 0.9301\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2874 - accuracy: 0.9356\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.2798 - accuracy: 0.9347\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2740 - accuracy: 0.9319\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2707 - accuracy: 0.9338\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2631 - accuracy: 0.9374\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.2638 - accuracy: 0.9347\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2527 - accuracy: 0.9392\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2497 - accuracy: 0.9428\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2417 - accuracy: 0.9419\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2399 - accuracy: 0.9465\n",
      "Epoch 83/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.2341 - accuracy: 0.9501\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.2248 - accuracy: 0.9483\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.2219 - accuracy: 0.9537\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2148 - accuracy: 0.9501\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2088 - accuracy: 0.9465\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2090 - accuracy: 0.9537\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.2014 - accuracy: 0.9546\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.1960 - accuracy: 0.9583\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.1934 - accuracy: 0.9583\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.1880 - accuracy: 0.9610\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.1875 - accuracy: 0.9592\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.1797 - accuracy: 0.9601\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.1773 - accuracy: 0.9655\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.1735 - accuracy: 0.9664\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.1688 - accuracy: 0.9682\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.1657 - accuracy: 0.9664\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.1624 - accuracy: 0.9610\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.1705 - accuracy: 0.9637\n"
     ]
    }
   ],
   "source": [
    "# creating the model\n",
    "\n",
    "# Input Layer\n",
    "i=Input(shape=(input_shape,))\n",
    "# Embedding Layer\n",
    "x=Embedding(vocabulary+1,10)(i)\n",
    "# LSTM Layer\n",
    "x=LSTM(10,return_sequences=True)(x)\n",
    "# Flatten Layer for converting array to 1D\n",
    "x=Flatten()(x)\n",
    "# Dense layer to output the resut by applying activation function\n",
    "x=Dense(output_length,activation=\"softmax\")(x)\n",
    "# Final model after implementing all layers\n",
    "model=Model(i,x)\n",
    "\n",
    "# compiling the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# training the model\n",
    "train=model.fit(x_train,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b1784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User : hello\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Saarthi :  Great to see you. How do you feel currently? \n",
      "\n",
      "User : great\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Saarthi :  I'm listening. Please go on. \n",
      "\n",
      "User : good morning\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Saarthi :  How were you feeling last week? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing on the model\n",
    "import random \n",
    "\n",
    "while True:\n",
    "    # Taking user input\n",
    "    texts_p=[]\n",
    "    pred_input=input('User : ')\n",
    "    \n",
    "    # Preprocess the same user input\n",
    "    pred_input=[letters.lower() for letters in pred_input if letters not in string.punctuation]\n",
    "    pred_input=''.join(pred_input)\n",
    "    texts_p.append(pred_input)\n",
    "    \n",
    "    # Tokenize and add padding in the user input\n",
    "    pred_input=tokenizer.texts_to_sequences(texts_p)\n",
    "    pred_input=np.array(pred_input).reshape(-1)\n",
    "    pred_input=pad_sequences([pred_input],input_shape)\n",
    "    \n",
    "    # Predict output from model\n",
    "    output=model.predict(pred_input)\n",
    "    output=output.argmax()\n",
    "    \n",
    "    # Fetch text corresponding to output number\n",
    "    res_tag=le.inverse_transform([output])[0]\n",
    "\n",
    "    # Print the result\n",
    "    print(\"Saarthi : \",random.choice(responses[res_tag]),\"\\n\")\n",
    "    if res_tag==\"goodbye\":\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
